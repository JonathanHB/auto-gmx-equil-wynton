                      :-) GROMACS - gmx mdrun, 2020.6 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf      Artem Zhmurov   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2020.6
Executable:   /wynton/home/grabe/shared/gromacs/gromacs-2020.6_CUDA10_SSE4/bin/gmx
Data prefix:  /wynton/home/grabe/shared/gromacs/gromacs-2020.6_CUDA10_SSE4
Working dir:  /wynton/home/grabe/jborowsky/aac1/westpa-16/traj_segs/000076/000866
Process ID:   41250
Command line:
  gmx mdrun -s seg.tpr -o seg.trr -c seg.gro -e seg.edr -cpi state.cpt -g seg.log -nb gpu -pme gpu -bonded gpu -maxh 2 -cpt 10 -nt 8 -gpu_id 0 -ntmpi 1

GROMACS version:    2020.6
Verified release checksum is 2f568d8884e039acbc6b68722432516e0628be00c847969b7c905c8b53ef826f
Precision:          single
Memory model:       64 bit
MPI library:        thread_mpi
OpenMP support:     enabled (GMX_OPENMP_MAX_THREADS = 64)
GPU support:        CUDA
SIMD instructions:  SSE4.1
FFT library:        fftw-3.3.8-sse2
RDTSCP usage:       disabled
TNG support:        enabled
Hwloc support:      disabled
Tracing support:    disabled
C compiler:         /opt/rh/devtoolset-7/root/usr/bin/cc GNU 7.3.1
C compiler flags:   -msse4.1 -fexcess-precision=fast -funroll-all-loops -O3 -DNDEBUG
C++ compiler:       /opt/rh/devtoolset-7/root/usr/bin/c++ GNU 7.3.1
C++ compiler flags: -msse4.1 -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG
CUDA compiler:      /salilab/diva1/programs/x86_64linux/cuda-10.0.130/lib64/cuda/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver;Copyright (c) 2005-2018 NVIDIA Corporation;Built on Sat_Aug_25_21:08:01_CDT_2018;Cuda compilation tools, release 10.0, V10.0.130
CUDA compiler flags:-gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_35,code=compute_35;-gencode;arch=compute_50,code=compute_50;-gencode;arch=compute_52,code=compute_52;-gencode;arch=compute_60,code=compute_60;-gencode;arch=compute_61,code=compute_61;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75;-use_fast_math;;-msse4.1 -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG
CUDA driver:        11.50
CUDA runtime:       10.0


Running on 1 node with total 32 cores, 64 logical cores, 1 compatible GPU
Hardware detected:
  CPU info:
    Vendor: AMD
    Brand:  AMD EPYC 7543P 32-Core Processor               
    Family: 25   Model: 1   Stepping: 1
    Features: aes amd apic avx avx2 clfsh cmov cx8 cx16 f16c fma htt lahf misalignsse mmx msr nonstop_tsc pcid pclmuldq pdpe1gb popcnt pse rdrnd rdtscp sha sse2 sse3 sse4a sse4.1 sse4.2 ssse3
  Hardware topology: Basic
    Sockets, cores, and logical processors:
      Socket  0: [   0  32] [   1  33] [   2  34] [   3  35] [   4  36] [   5  37] [   6  38] [   7  39] [   8  40] [   9  41] [  10  42] [  11  43] [  12  44] [  13  45] [  14  46] [  15  47] [  16  48] [  17  49] [  18  50] [  19  51] [  20  52] [  21  53] [  22  54] [  23  55] [  24  56] [  25  57] [  26  58] [  27  59] [  28  60] [  29  61] [  30  62] [  31  63]
  GPU info:
    Number of GPUs detected: 1
    #0: NVIDIA NVIDIA A40, compute cap.: 8.6, ECC: yes, stat: compatible

Highest SIMD level requested by all nodes in run: AVX2_256
SIMD instructions selected at compile time:       SSE4.1
This program was compiled for different hardware than you are running on,
which could influence performance.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
M. J. Abraham, T. Murtola, R. Schulz, S. Páll, J. C. Smith, B. Hess, E.
Lindahl
GROMACS: High performance molecular simulations through multi-level
parallelism from laptops to supercomputers
SoftwareX 1 (2015) pp. 19-25
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Páll, M. J. Abraham, C. Kutzner, B. Hess, E. Lindahl
Tackling Exascale Software Challenges in Molecular Dynamics Simulations with
GROMACS
In S. Markidis & E. Laure (Eds.), Solving Software Challenges for Exascale 8759 (2015) pp. 3-27
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Pronk, S. Páll, R. Schulz, P. Larsson, P. Bjelkmar, R. Apostolov, M. R.
Shirts, J. C. Smith, P. M. Kasson, D. van der Spoel, B. Hess, and E. Lindahl
GROMACS 4.5: a high-throughput and highly parallel open source molecular
simulation toolkit
Bioinformatics 29 (2013) pp. 845-54
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
B. Hess and C. Kutzner and D. van der Spoel and E. Lindahl
GROMACS 4: Algorithms for highly efficient, load-balanced, and scalable
molecular simulation
J. Chem. Theory Comput. 4 (2008) pp. 435-447
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
D. van der Spoel, E. Lindahl, B. Hess, G. Groenhof, A. E. Mark and H. J. C.
Berendsen
GROMACS: Fast, Flexible and Free
J. Comp. Chem. 26 (2005) pp. 1701-1719
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
E. Lindahl and B. Hess and D. van der Spoel
GROMACS 3.0: A package for molecular simulation and trajectory analysis
J. Mol. Mod. 7 (2001) pp. 306-317
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
H. J. C. Berendsen, D. van der Spoel and R. van Drunen
GROMACS: A message-passing parallel molecular dynamics implementation
Comp. Phys. Comm. 91 (1995) pp. 43-56
-------- -------- --- Thank You --- -------- --------


++++ PLEASE CITE THE DOI FOR THIS VERSION OF GROMACS ++++
https://doi.org/10.5281/zenodo.4576055
-------- -------- --- Thank You --- -------- --------


The number of OpenMP threads was set by environment variable OMP_NUM_THREADS to 8

This run will default to '-update gpu' as requested by the GMX_FORCE_UPDATE_DEFAULT_GPU environment variable. GPU update with domain decomposition lacks substantial testing and should be used with caution.

Enabling GPU buffer operations required by GMX_GPU_DD_COMMS (equivalent with GMX_USE_GPU_BUFFER_OPS=1).

This run uses the 'GPU halo exchange' feature, enabled by the GMX_GPU_DD_COMMS environment variable.

This run uses the 'GPU PME-PP communications' feature, enabled by the GMX_GPU_PME_PP_COMMS environment variable.
Input Parameters:
   integrator                     = md
   tinit                          = 0
   dt                             = 0.002
   nsteps                         = 10000
   init-step                      = 0
   simulation-part                = 1
   comm-mode                      = Linear
   nstcomm                        = 500
   bd-fric                        = 0
   ld-seed                        = -1611284497
   emtol                          = 10
   emstep                         = 0.01
   niter                          = 20
   fcstep                         = 0
   nstcgsteep                     = 1000
   nbfgscorr                      = 10
   rtpi                           = 0.05
   nstxout                        = 10000
   nstvout                        = 10000
   nstfout                        = 10000
   nstlog                         = 5000
   nstcalcenergy                  = 500
   nstenergy                      = 5000
   nstxout-compressed             = 10000
   compressed-x-precision         = 1000
   cutoff-scheme                  = Verlet
   nstlist                        = 20
   pbc                            = xyz
   periodic-molecules             = false
   verlet-buffer-tolerance        = 0.005
   rlist                          = 1.206
   coulombtype                    = PME
   coulomb-modifier               = Potential-shift
   rcoulomb-switch                = 0
   rcoulomb                       = 1.2
   epsilon-r                      = 1
   epsilon-rf                     = inf
   vdw-type                       = Cut-off
   vdw-modifier                   = Force-switch
   rvdw-switch                    = 1
   rvdw                           = 1.2
   DispCorr                       = No
   table-extension                = 1
   fourierspacing                 = 0.12
   fourier-nx                     = 84
   fourier-ny                     = 84
   fourier-nz                     = 96
   pme-order                      = 4
   ewald-rtol                     = 1e-05
   ewald-rtol-lj                  = 0.001
   lj-pme-comb-rule               = Geometric
   ewald-geometry                 = 0
   epsilon-surface                = 0
   tcoupl                         = V-rescale
   nsttcouple                     = 20
   nh-chain-length                = 0
   print-nose-hoover-chain-variables = false
   pcoupl                         = Parrinello-Rahman
   pcoupltype                     = Semiisotropic
   nstpcouple                     = 20
   tau-p                          = 5
   compressibility (3x3):
      compressibility[    0]={ 4.50000e-05,  0.00000e+00,  0.00000e+00}
      compressibility[    1]={ 0.00000e+00,  4.50000e-05,  0.00000e+00}
      compressibility[    2]={ 0.00000e+00,  0.00000e+00,  4.50000e-05}
   ref-p (3x3):
      ref-p[    0]={ 1.01325e+00,  0.00000e+00,  0.00000e+00}
      ref-p[    1]={ 0.00000e+00,  1.01325e+00,  0.00000e+00}
      ref-p[    2]={ 0.00000e+00,  0.00000e+00,  1.01325e+00}
   refcoord-scaling               = COM
   posres-com (3):
      posres-com[0]= 0.00000e+00
      posres-com[1]= 0.00000e+00
      posres-com[2]= 0.00000e+00
   posres-comB (3):
      posres-comB[0]= 0.00000e+00
      posres-comB[1]= 0.00000e+00
      posres-comB[2]= 0.00000e+00
   QMMM                           = false
   QMconstraints                  = 0
   QMMMscheme                     = 0
   MMChargeScaleFactor            = 1
qm-opts:
   ngQM                           = 0
   constraint-algorithm           = Lincs
   continuation                   = true
   Shake-SOR                      = false
   shake-tol                      = 0.0001
   lincs-order                    = 4
   lincs-iter                     = 1
   lincs-warnangle                = 30
   nwall                          = 0
   wall-type                      = 9-3
   wall-r-linpot                  = -1
   wall-atomtype[0]               = -1
   wall-atomtype[1]               = -1
   wall-density[0]                = 0
   wall-density[1]                = 0
   wall-ewald-zfac                = 3
   pull                           = false
   awh                            = false
   rotation                       = false
   interactiveMD                  = false
   disre                          = No
   disre-weighting                = Conservative
   disre-mixed                    = false
   dr-fc                          = 1000
   dr-tau                         = 0
   nstdisreout                    = 100
   orire-fc                       = 0
   orire-tau                      = 0
   nstorireout                    = 100
   free-energy                    = no
   cos-acceleration               = 0
   deform (3x3):
      deform[    0]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      deform[    1]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      deform[    2]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
   simulated-tempering            = false
   swapcoords                     = no
   userint1                       = 0
   userint2                       = 0
   userint3                       = 0
   userint4                       = 0
   userreal1                      = 0
   userreal2                      = 0
   userreal3                      = 0
   userreal4                      = 0
   applied-forces:
     electric-field:
       x:
         E0                       = 0
         omega                    = 0
         t0                       = 0
         sigma                    = 0
       y:
         E0                       = 0
         omega                    = 0
         t0                       = 0
         sigma                    = 0
       z:
         E0                       = 0
         omega                    = 0
         t0                       = 0
         sigma                    = 0
     density-guided-simulation:
       active                     = false
       group                      = protein
       similarity-measure         = inner-product
       atom-spreading-weight      = unity
       force-constant             = 1e+09
       gaussian-transform-spreading-width = 0.2
       gaussian-transform-spreading-range-in-multiples-of-width = 4
       reference-density-filename = reference.mrc
       nst                        = 1
       normalize-densities        = true
       adaptive-force-scaling     = false
       adaptive-force-scaling-time-constant = 4
grpopts:
   nrdf:       94206      116199
   ref-t:      310.15      310.15
   tau-t:           1           1
annealing:          No          No
annealing-npoints:           0           0
   acc:	           0           0           0
   nfreeze:           N           N           N
   energygrp-flags[  0]: 0

Changing nstlist from 20 to 100, rlist from 1.206 to 1.321


1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the GPU
PME tasks will do all aspects on the GPU
Using 1 MPI thread

Non-default thread affinity set, disabling internal thread affinity

Using 8 OpenMP threads 

System total charge: 0.000
Will do PME sum in reciprocal space for electrostatic interactions.

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
U. Essmann, L. Perera, M. L. Berkowitz, T. Darden, H. Lee and L. G. Pedersen 
A smooth particle mesh Ewald method
J. Chem. Phys. 103 (1995) pp. 8577-8592
-------- -------- --- Thank You --- -------- --------

Using a Gaussian width (1/beta) of 0.384195 nm for Ewald
Potential shift: LJ r^-12: -2.648e-01 r^-6: -5.349e-01, Ewald -8.333e-06
Initialized non-bonded Ewald tables, spacing: 1.02e-03 size: 1176

Generated table with 1160 data points for 1-4 COUL.
Tabscale = 500 points/nm
Generated table with 1160 data points for 1-4 LJ6.
Tabscale = 500 points/nm
Generated table with 1160 data points for 1-4 LJ12.
Tabscale = 500 points/nm

Using GPU 8x8 nonbonded short-range kernels

Using a dual 8x8 pair-list setup updated with dynamic, rolling pruning:
  outer list: updated every 100 steps, buffer 0.121 nm, rlist 1.321 nm
  inner list: updated every  18 steps, buffer 0.005 nm, rlist 1.205 nm
At tolerance 0.005 kJ/mol/ps per atom, equivalent classical 1x1 list would be:
  outer list: updated every 100 steps, buffer 0.280 nm, rlist 1.480 nm
  inner list: updated every  18 steps, buffer 0.078 nm, rlist 1.278 nm


Initializing LINear Constraint Solver

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
B. Hess and H. Bekker and H. J. C. Berendsen and J. G. E. M. Fraaije
LINCS: A Linear Constraint Solver for molecular simulations
J. Comp. Chem. 18 (1997) pp. 1463-1472
-------- -------- --- Thank You --- -------- --------

The number of constraints is 23529

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Miyamoto and P. A. Kollman
SETTLE: An Analytical Version of the SHAKE and RATTLE Algorithms for Rigid
Water Models
J. Comp. Chem. 13 (1992) pp. 952-962
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
G. Bussi, D. Donadio and M. Parrinello
Canonical sampling through velocity rescaling
J. Chem. Phys. 126 (2007) pp. 014101
-------- -------- --- Thank You --- -------- --------

There are: 97294 Atoms

Updating coordinates and applying constraints on the GPU.
Center of mass motion removal mode is Linear
We have the following groups for center of mass motion removal:
  0:  Protein_TLCL2_DNF_POPC
  1:  SOD_CLA_TIP3

Started mdrun on rank 0 Sat Oct 21 18:28:02 2023

           Step           Time
              0        0.00000

   Energies (kJ/mol)
           Bond            U-B    Proper Dih.  Improper Dih.      CMAP Dih.
    1.93289e+04    9.78001e+04    6.86124e+04    1.19723e+03   -1.17022e+02
          LJ-14     Coulomb-14        LJ (SR)   Coulomb (SR)   Coul. recip.
    1.23835e+04   -6.25735e+04    1.64110e+04   -9.94274e+05    4.51686e+03
      Potential    Kinetic En.   Total Energy  Conserved En.    Temperature
   -8.36715e+05    2.72931e+05   -5.63784e+05   -5.63726e+05    3.12027e+02
 Pressure (bar)   Constr. rmsd
   -2.27360e+02    0.00000e+00

           Step           Time
           5000       10.00000

   Energies (kJ/mol)
           Bond            U-B    Proper Dih.  Improper Dih.      CMAP Dih.
    1.91866e+04    9.64913e+04    6.87678e+04    1.31590e+03   -7.16739e+01
          LJ-14     Coulomb-14        LJ (SR)   Coulomb (SR)   Coul. recip.
    1.22419e+04   -6.30595e+04    1.81419e+04   -9.94878e+05    4.64471e+03
      Potential    Kinetic En.   Total Energy  Conserved En.    Temperature
   -8.37219e+05    2.70428e+05   -5.66791e+05   -5.64095e+05    3.09166e+02
 Pressure (bar)   Constr. rmsd
    4.09739e-01    0.00000e+00

step 5100: timed with pme grid 84 84 96, coulomb cutoff 1.200: 281.3 M-cycles
step 5300: timed with pme grid 80 80 84, coulomb cutoff 1.210: 283.4 M-cycles
step 5500: timed with pme grid 72 72 80, coulomb cutoff 1.339: 342.9 M-cycles
step 5700: timed with pme grid 80 80 80, coulomb cutoff 1.271: 303.1 M-cycles
step 5900: timed with pme grid 80 80 84, coulomb cutoff 1.210: 284.9 M-cycles
step 6100: timed with pme grid 80 80 96, coulomb cutoff 1.205: 289.1 M-cycles
step 6300: timed with pme grid 84 84 96, coulomb cutoff 1.200: 281.3 M-cycles
              optimal pme grid 84 84 96, coulomb cutoff 1.200
           Step           Time
          10000       20.00000

Writing checkpoint, step 10000 at Sat Oct 21 18:28:15 2023


   Energies (kJ/mol)
           Bond            U-B    Proper Dih.  Improper Dih.      CMAP Dih.
    1.89853e+04    9.71780e+04    6.82918e+04    1.21045e+03   -1.21349e+02
          LJ-14     Coulomb-14        LJ (SR)   Coulomb (SR)   Coul. recip.
    1.22720e+04   -6.21451e+04    1.60324e+04   -9.95152e+05    4.38280e+03
      Potential    Kinetic En.   Total Energy  Conserved En.    Temperature
   -8.39065e+05    2.72039e+05   -5.67027e+05   -5.63678e+05    3.11007e+02
 Pressure (bar)   Constr. rmsd
   -6.80720e+01    0.00000e+00


	<======  ###############  ==>
	<====  A V E R A G E S  ====>
	<==  ###############  ======>

	Statistics over 10001 steps using 21 frames

   Energies (kJ/mol)
           Bond            U-B    Proper Dih.  Improper Dih.      CMAP Dih.
    1.90935e+04    9.70272e+04    6.84580e+04    1.22640e+03   -1.45718e+02
          LJ-14     Coulomb-14        LJ (SR)   Coulomb (SR)   Coul. recip.
    1.24073e+04   -6.24229e+04    1.79536e+04   -9.96349e+05    4.45289e+03
      Potential    Kinetic En.   Total Energy  Conserved En.    Temperature
   -8.38299e+05    2.71367e+05   -5.66932e+05   -5.64045e+05    3.10239e+02
 Pressure (bar)   Constr. rmsd
    9.33552e+00    0.00000e+00

          Box-X          Box-Y          Box-Z
    9.65004e+00    9.65004e+00    1.01354e+01

   Total Virial (kJ/mol)
    9.00538e+04   -2.92827e+02    7.11495e+01
   -2.92755e+02    8.92586e+04    2.49955e+02
    7.11844e+01    2.49869e+02    9.12578e+04

   Pressure (bar)
   -2.62723e+01    1.66143e+01   -6.62097e-01
    1.66118e+01    4.36825e+00   -7.92542e+00
   -6.63322e-01   -7.92240e+00    4.99106e+01

T-Protein_TLCL2_DNF_POPC T-SOD_CLA_TIP3
    3.10855e+02    3.09740e+02


	M E G A - F L O P S   A C C O U N T I N G

 NB=Group-cutoff nonbonded kernels    NxN=N-by-N cluster Verlet kernels
 RF=Reaction-Field  VdW=Van der Waals  QSTab=quadratic-spline table
 W3=SPC/TIP3p  W4=TIP4p (single or pairs)
 V&F=Potential and force  V=Potential only  F=Force only

 Computing:                               M-Number         M-Flops  % Flops
-----------------------------------------------------------------------------
 Pair Search distance check            1176.912480       10592.212     0.0
 NxN Ewald Elec. + LJ [F]           1234792.904256    96313846.532    99.1
 NxN Ewald Elec. + LJ [V&F]            2592.353152      334413.557     0.3
 1,4 nonbonded interactions            1041.264116       93713.770     0.1
 Shift-X                                  9.826694          58.960     0.0
 Bonds                                  155.045503        9147.685     0.0
 Propers                               1246.164604      285371.694     0.3
 Impropers                               12.181218        2533.693     0.0
 Virial                                  48.766839         877.803     0.0
 Stop-CM                                  2.043174          20.432     0.0
 Calc-Ekin                               97.488588        2632.192     0.0
 CMAP                                     2.880288        4896.490     0.0
 Urey-Bradley                           746.004593      136518.841     0.1
-----------------------------------------------------------------------------
 Total                                                97194623.861   100.0
-----------------------------------------------------------------------------


     R E A L   C Y C L E   A N D   T I M E   A C C O U N T I N G

On 1 MPI rank, each using 8 OpenMP threads

 Computing:          Num   Num      Call    Wall time         Giga-Cycles
                     Ranks Threads  Count      (s)         total sum    %
-----------------------------------------------------------------------------
 Neighbor search        1    8        101       1.001         22.389   7.4
 Launch GPU ops.        1    8      10001       0.700         15.655   5.2
 Force                  1    8      10001       0.381          8.508   2.8
 Wait PME GPU gather    1    8      10001       0.045          1.017   0.3
 Wait Bonded GPU        1    8         21       0.000          0.001   0.0
 Reduce GPU PME F       1    8      10001       0.014          0.310   0.1
 Wait GPU NB local      1    8       9500       0.053          1.189   0.4
 Wait GPU state copy    1    8      11344       6.723        150.309  49.5
 NB X/F buffer ops.     1    8      20002       0.147          3.289   1.1
 Write traj.            1    8          2       0.418          9.342   3.1
 Rest                                           4.104         91.747  30.2
-----------------------------------------------------------------------------
 Total                                         13.587        303.756 100.0
-----------------------------------------------------------------------------

               Core t (s)   Wall t (s)        (%)
       Time:      108.655       13.587      799.7
                 (ns/day)    (hour/ns)
Performance:      127.195        0.189
Finished mdrun on rank 0 Sat Oct 21 18:28:16 2023

